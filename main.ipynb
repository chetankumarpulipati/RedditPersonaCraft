{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fea58e7fdbfdb6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.53.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: praw in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: requests in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cheta\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\cheta\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers python-dotenv praw tqdm torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed1db429a5205f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flask app is running. Access it at http://127.0.0.1:5000/persona/\n",
      " * Serving Flask app '__main__'\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quitPress CTRL+C to quit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, url_for, send_from_directory, redirect\n",
    "import os\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/persona/')\n",
    "def list_personas():\n",
    "    persona_files = [f for f in os.listdir('output') if f.endswith('.html')]\n",
    "    users = [f.replace('.html', '') for f in persona_files]\n",
    "    links = [f\"<li><a href='/persona/html/{user}' target='_blank'>{user}</a></li>\" for user in users]\n",
    "    return f\"<h2>Available Personas</h2><ul>{''.join(links)}</ul>\"\n",
    "\n",
    "@app.route('/persona/html/<username>')\n",
    "def serve_persona_html(username):\n",
    "    html_filename = f'{username}.html'\n",
    "    html_path = os.path.join('output', html_filename)\n",
    "    if not os.path.exists(html_path):\n",
    "        return f\"Persona HTML for {username} not found.\"\n",
    "    return send_from_directory('output', html_filename)\n",
    "\n",
    "def run_flask():\n",
    "    app.run(debug=True, use_reloader=False)\n",
    "\n",
    "# Start Flask app in a background thread\n",
    "threading.Thread(target=run_flask).start()\n",
    "print(\"Flask app is running. Access it at http://127.0.0.1:5000/persona/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e94b4509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit scraper functions loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Reddit Scraper Implementation\n",
    "import praw\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_user_data(username):\n",
    "    \"\"\"\n",
    "    Fetch user data from Reddit using multiple methods\n",
    "    Returns: posts, comments, profile_info\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try with PRAW if credentials are available\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "            client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "            user_agent=os.getenv(\"REDDIT_USER_AGENT\", \"RedditPersonaCraft/1.0 by /u/yourname\")\n",
    "        )\n",
    "        \n",
    "        print(f\"[1] Fetching data for Reddit user: {username}\")\n",
    "        \n",
    "        # Get user profile\n",
    "        user = reddit.redditor(username)\n",
    "        \n",
    "        # Extract profile info\n",
    "        profile_info = {\n",
    "            \"name\": user.name,\n",
    "            \"icon_img\": getattr(user, 'icon_img', '') or \"https://www.redditstatic.com/avatars/avatar_default_02_25B79F.png\",\n",
    "            \"created_utc\": user.created_utc,\n",
    "            \"total_karma\": getattr(user, 'total_karma', 0),\n",
    "            \"link_karma\": user.link_karma,\n",
    "            \"comment_karma\": user.comment_karma,\n",
    "            \"subreddit\": getattr(user.subreddit, 'display_name', '') if hasattr(user, 'subreddit') and user.subreddit else \"\",\n",
    "            \"bio\": getattr(user, 'subreddit', {}).get('public_description', '') if hasattr(user, 'subreddit') and user.subreddit else \"\"\n",
    "        }\n",
    "        \n",
    "        # Fetch posts\n",
    "        posts = []\n",
    "        print(\"[2] Fetching posts...\")\n",
    "        try:\n",
    "            for post in tqdm(user.submissions.new(limit=50)):\n",
    "                posts.append({\n",
    "                    \"type\": \"post\",\n",
    "                    \"title\": post.title,\n",
    "                    \"selftext\": post.selftext,\n",
    "                    \"url\": post.url,\n",
    "                    \"subreddit\": post.subreddit.display_name,\n",
    "                    \"created_utc\": post.created_utc\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching posts: {e}\")\n",
    "        \n",
    "        # Fetch comments\n",
    "        comments = []\n",
    "        print(\"[3] Fetching comments...\")\n",
    "        try:\n",
    "            for comment in tqdm(user.comments.new(limit=100)):\n",
    "                comments.append({\n",
    "                    \"type\": \"comment\",\n",
    "                    \"body\": comment.body,\n",
    "                    \"subreddit\": comment.subreddit.display_name,\n",
    "                    \"created_utc\": comment.created_utc,\n",
    "                    \"link_url\": f\"https://www.reddit.com{comment.permalink}\"\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments: {e}\")\n",
    "        \n",
    "        print(f\"[4] Successfully fetched {len(posts)} posts and {len(comments)} comments using PRAW\")\n",
    "        return posts, comments, profile_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"PRAW method failed: {e}\")\n",
    "        print(\"[5] Falling back to web scraping method...\")\n",
    "        \n",
    "        # Fallback to web scraping\n",
    "        return fetch_user_data_web_scraping(username)\n",
    "\n",
    "def fetch_user_data_web_scraping(username):\n",
    "    \"\"\"\n",
    "    Fallback method using web scraping when PRAW fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Basic profile info\n",
    "        profile_info = {\n",
    "            \"name\": username,\n",
    "            \"icon_img\": \"https://www.redditstatic.com/avatars/avatar_default_02_25B79F.png\",\n",
    "            \"created_utc\": time.time(),\n",
    "            \"total_karma\": 0,\n",
    "            \"link_karma\": 0,\n",
    "            \"comment_karma\": 0,\n",
    "            \"subreddit\": \"\",\n",
    "            \"bio\": \"\"\n",
    "        }\n",
    "        \n",
    "        # Try to fetch some basic data from Reddit's JSON API\n",
    "        headers = {\n",
    "            'User-Agent': 'RedditPersonaCraft/1.0'\n",
    "        }\n",
    "        \n",
    "        posts = []\n",
    "        comments = []\n",
    "        \n",
    "        try:\n",
    "            # Fetch user's posts\n",
    "            posts_url = f\"https://www.reddit.com/user/{username}/submitted/.json?limit=50\"\n",
    "            response = requests.get(posts_url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                for post_data in data.get('data', {}).get('children', []):\n",
    "                    post = post_data.get('data', {})\n",
    "                    posts.append({\n",
    "                        \"type\": \"post\",\n",
    "                        \"title\": post.get('title', ''),\n",
    "                        \"selftext\": post.get('selftext', ''),\n",
    "                        \"url\": post.get('url', ''),\n",
    "                        \"subreddit\": post.get('subreddit', ''),\n",
    "                        \"created_utc\": post.get('created_utc', time.time())\n",
    "                    })\n",
    "            \n",
    "            # Fetch user's comments\n",
    "            comments_url = f\"https://www.reddit.com/user/{username}/comments/.json?limit=100\"\n",
    "            response = requests.get(comments_url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                for comment_data in data.get('data', {}).get('children', []):\n",
    "                    comment = comment_data.get('data', {})\n",
    "                    comments.append({\n",
    "                        \"type\": \"comment\",\n",
    "                        \"body\": comment.get('body', ''),\n",
    "                        \"subreddit\": comment.get('subreddit', ''),\n",
    "                        \"created_utc\": comment.get('created_utc', time.time()),\n",
    "                        \"link_url\": f\"https://www.reddit.com{comment.get('permalink', '')}\"\n",
    "                    })\n",
    "            \n",
    "            print(f\"[6] Web scraping fetched {len(posts)} posts and {len(comments)} comments\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Web scraping also failed: {e}\")\n",
    "            print(\"Using sample data for demonstration...\")\n",
    "            \n",
    "            # If all else fails, use sample data\n",
    "            posts = [\n",
    "                {\n",
    "                    \"type\": \"post\",\n",
    "                    \"title\": \"Sample Post Title\",\n",
    "                    \"selftext\": \"This is a sample post to demonstrate the persona building functionality.\",\n",
    "                    \"url\": \"https://www.reddit.com/r/sample\",\n",
    "                    \"subreddit\": \"sample\",\n",
    "                    \"created_utc\": time.time()\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            comments = [\n",
    "                {\n",
    "                    \"type\": \"comment\",\n",
    "                    \"body\": \"This is a sample comment to demonstrate the persona building functionality.\",\n",
    "                    \"subreddit\": \"sample\",\n",
    "                    \"created_utc\": time.time(),\n",
    "                    \"link_url\": \"https://www.reddit.com/r/sample/comments/sample\"\n",
    "                }\n",
    "            ]\n",
    "        \n",
    "        return posts, comments, profile_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"All methods failed: {e}\")\n",
    "        return [], [], {\"name\": username, \"icon_img\": \"\", \"created_utc\": time.time(), \"total_karma\": 0, \"link_karma\": 0, \"comment_karma\": 0, \"subreddit\": \"\", \"bio\": \"\"}\n",
    "\n",
    "print(\"Reddit scraper functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25dcd1ea8c297c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted username: Hungry-Move-6603\n",
      "[1] Fetching data for Reddit user: Hungry-Move-6603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheta\\AppData\\Local\\Temp\\ipykernel_15056\\4268133347.py:36: DeprecationWarning: 'Redditor.subreddit' is no longer a dict and is now an UserSubreddit object. Using 'get' is deprecated and will be removed in PRAW 8.\n",
      "  \"bio\": getattr(user, 'subreddit', {}).get('public_description', '') if hasattr(user, 'subreddit') and user.subreddit else \"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] Fetching posts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  7.15it/s]\n",
      "3it [00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] Fetching comments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:00, 24.72it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] Successfully fetched 3 posts and 12 comments using PRAW\n",
      "Posts fetched: 3 | Comments fetched: 12\n",
      "Scraped data stored in temp.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7] Saving persona to output directory...\n",
      "Persona saved to output/Hungry-Move-6603_persona.txt\n",
      "Persona HTML generated as output/Hungry-Move-6603.html using sample1.html template.\n",
      "[8] Script completed successfully.\n"
     ]
    }
   ],
   "source": [
    "#persona_builder\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LLM_PROVIDER = os.getenv(\"LLM_PROVIDER\", \"transformers\")\n",
    "MODEL_NAME = os.getenv(\"TRANSFORMERS_MODEL\", \"distilgpt2\")\n",
    "\n",
    "def chunk_texts(texts, max_chars=1500):\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    for t in texts:\n",
    "        if len(current) + len(t) < max_chars:\n",
    "            current += t + \"\\n\\n\"\n",
    "        else:\n",
    "            chunks.append(current)\n",
    "            current = t + \"\\n\\n\"\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "    return chunks\n",
    "\n",
    "def build_persona(posts, comments):\n",
    "    all_texts = posts + comments\n",
    "    chunks = chunk_texts(all_texts, max_chars=1500)\n",
    "    system_prompt = (\n",
    "        \"You are a helpful AI assistant. Analyze the following Reddit posts and comments to generate a detailed user persona. \"\n",
    "        \"Predict and fill in the following fields based on the user's activity. Format your output as follows:\\n\"\n",
    "        \"Name: <predicted name>\\n\"\n",
    "        \"Age: <predicted age>\\n\"\n",
    "        \"Occupation: <predicted occupation>\\n\"\n",
    "        \"Status: <predicted status>\\n\"\n",
    "        \"Location: <predicted location>\\n\"\n",
    "        \"Tier: <predicted tier>\\n\"\n",
    "        \"Archetype: <predicted archetype>\\n\"\n",
    "        \"\\nTraits:\\n<list traits, one per line>\\n\"\n",
    "        \"\\nMotivations:\\n<list motivations, one per line>\\n\"\n",
    "        \"\\nPersonality:\\n<list personality attributes, one per line>\\n\"\n",
    "        \"\\nBehaviour & Habits:\\n<list habits, one per line>\\n\"\n",
    "        \"\\nGoals & Needs:\\n<list goals, one per line>\\n\"\n",
    "        \"\\nFrustrations:\\n<list frustrations, one per line>\\n\"\n",
    "        \"\\nBase your predictions on the user's posts and comments. If information is missing, make a reasonable guess based on context.\"\n",
    "    )\n",
    "    generator = pipeline(\"text-generation\", model=MODEL_NAME)\n",
    "    persona_dict = {}\n",
    "    for chunk in chunks:\n",
    "        prompt = f\"{system_prompt}\\n\\n{chunk}\"\n",
    "        try:\n",
    "            result = generator(prompt, max_length=1024, do_sample=True, temperature=0.7)[0]['generated_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating persona chunk: {e}\")\n",
    "            continue\n",
    "        import re\n",
    "        required_fields = [\n",
    "            'Name:', 'Age:', 'Occupation:', 'Status:', 'Location:', 'Tier:', 'Archetype:',\n",
    "            'Traits:', 'Motivations:', 'Personality:', 'Behaviour & Habits:', 'Goals & Needs:', 'Frustrations:'\n",
    "        ]\n",
    "        for field in required_fields:\n",
    "            match = re.search(rf'{field}\\s*(.*)', result)\n",
    "            if match:\n",
    "                value = match.group(1).strip()\n",
    "                if not value or value.startswith('<') or value.lower().startswith('list') or value.startswith('(AI guess'):\n",
    "                    persona_dict[field] = None\n",
    "                else:\n",
    "                    persona_dict[field] = value\n",
    "            else:\n",
    "                persona_dict[field] = None\n",
    "\n",
    "    persona_text = \"\"\n",
    "    for field in required_fields:\n",
    "        persona_text += f\"{field} {persona_dict[field]}\\n\"\n",
    "\n",
    "    # Add user's posts and comments to persona text\n",
    "    persona_text += \"\\n[User's Reddit Posts:]\\n\"\n",
    "    for post in posts:\n",
    "        persona_text += post + \"\\n\"\n",
    "    persona_text += \"\\n[User's Reddit Comments:]\\n\"\n",
    "    for comment in comments:\n",
    "        persona_text += comment + \"\\n\"\n",
    "    return persona_text\n",
    "\n",
    "reddit_url = input('Enter the Reddit profile URL (e.g., https://www.reddit.com/user/kojied): ')\n",
    "\n",
    "if reddit_url.startswith('https://www.reddit.com/user/'):\n",
    "    username = reddit_url.strip('/').split('/')[-1]\n",
    "    print(f\"Extracted username: {username}\")\n",
    "    posts, comments, profile_info = fetch_user_data(username)\n",
    "    print(f\"Posts fetched: {len(posts)} | Comments fetched: {len(comments)}\")\n",
    "    import json\n",
    "    temp_data = {\n",
    "        \"username\": username,\n",
    "        \"posts\": posts,\n",
    "        \"comments\": comments,\n",
    "        \"profile_info\": profile_info\n",
    "    }\n",
    "    with open(\"temp.json\", \"w\", encoding=\"utf-8\") as tempf:\n",
    "        json.dump(temp_data, tempf, ensure_ascii=False, indent=2)\n",
    "    print(\"Scraped data stored in temp.json\")\n",
    "    post_strs = [f\"Post: {p['title']}\\nText: {p['selftext']}\\nSubreddit: {p['subreddit']}\\nURL: {p['url']}\\nDate: {p['created_utc']}\" for p in posts]\n",
    "    comment_strs = [f\"Comment: {c['body']}\\nSubreddit: {c['subreddit']}\\nDate: {c['created_utc']}\" for c in comments]\n",
    "    persona = build_persona(post_strs, comment_strs)\n",
    "    print(\"[7] Saving persona to output directory...\")\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    output_path = f\"output/{username}_persona.txt\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(persona)\n",
    "    print(f\"Persona saved to {output_path}\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"update_persona_html.py\", username])\n",
    "    print(f\"Persona HTML generated as output/{username}.html using sample1.html template.\")\n",
    "    print(\"[8] Script completed successfully.\")\n",
    "else:\n",
    "    print(\"Please enter a valid Reddit profile URL (e.g., https://www.reddit.com/user/kojied)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c41a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "def render_list(items: List[str], tag: str = \"li\"):\n",
    "    return \"\\n\".join([f\"<{tag}>{item}</{tag}>\" for item in items])\n",
    "\n",
    "def update_html_template(template_path, output_path, persona_data):\n",
    "    with open(template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        html = f.read()\n",
    "    html = html.replace(\"Lucas Mellor\", persona_data.get(\"name\", \"\"))\n",
    "    html = html.replace(\"31\", persona_data.get(\"age\", \"\"))\n",
    "    html = html.replace(\"Content Manager\", persona_data.get(\"occupation\", \"\"))\n",
    "    html = html.replace(\"Single\", persona_data.get(\"status\", \"\"))\n",
    "    html = html.replace(\"London, UK\", persona_data.get(\"location\", \"\"))\n",
    "    html = html.replace(\"Early Adopters\", persona_data.get(\"tier\", \"\"))\n",
    "    html = html.replace(\"The Creator\", persona_data.get(\"archetype\", \"\"))\n",
    "    html = html.replace(\"“I want to spend less time ordering a healthy takeaway and more time enjoying my meal.”\", persona_data.get(\"quote\", \"\"))\n",
    "    html = html.replace(\"https://styles.redditmedia.com/t5_6zabzi/styles/communityIcon_m4kry55rs0m91.png?width=128&frame=1&auto=webp&s=c91a28d237fe9d3973fd33faec71ddfe4685f785\", persona_data.get(\"user_img\", \"\"))\n",
    "    trait_html = render_list(persona_data.get(\"traits\", []), tag=\"div class=\\\"trait-box\\\"\")\n",
    "    html = html.replace('<div class=\"trait-box\">Practical</div>\\n            <div class=\"trait-box\">Adaptable</div>\\n            <div class=\"trait-box\">Spontaneous</div>\\n            <div class=\"trait-box\">Active</div>', trait_html)\n",
    "    motivation_html = \"\".join([f'<div class=\"motivation-row\"><span><strong>{m}</strong></span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:80%\"></div></div></div>' for m in persona_data.get(\"motivations\", [])])\n",
    "    html = html.replace('<div class=\"motivation-row\"><span><strong>Convenience</strong></span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:100%\"></div></div></div>\\n        <div class=\"motivation-row\"><span><strong>Wellness</strong></span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:90%\"></div></div></div>\\n        <div class=\"motivation-row\"><span><strong>Speed</strong></span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:85%\"></div></div></div>\\n        <div class=\"motivation-row\"><span><strong>Preferences</strong></span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:70%\"></div></div></div>\\n        <div class=\"motivation-row\"><span><strong>Comfort</strong></span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:60%\"></div></div></div>\\n        <div class=\"motivation-row\"><span><strong>Dietary Needs</strong></span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:80%\"></div></div></div>', motivation_html)\n",
    "    personality_html = \"\".join([f'<div class=\"personality-row\"><span>{p}</span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:50%\"></div></div></div>' for p in persona_data.get(\"personality\", [])])\n",
    "    html = html.replace('<div class=\"personality-row\"><span>Introvert</span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:50%\"></div></div><span>Extrovert</span></div>\\n        <div class=\"personality-row\"><span>Intuition</span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:90%\"></div></div><span>Sensing</span></div>\\n        <div class=\"personality-row\"><span>Feeling</span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:30%\"></div></div><span>Thinking</span></div>\\n        <div class=\"personality-row\"><span>Perceiving</span><div class=\"bar\"><div class=\"bar-fill\" style=\"width:75%\"></div></div><span>Judging</span></div>', personality_html)\n",
    "    habits_html = render_list(persona_data.get(\"habits\", []))\n",
    "    html = html.replace('<li>Rarely cooked before lockdown</li>\\n          <li>Orders all meals online</li>\\n          <li>Joined online HIIT sessions</li>\\n          <li>Struggles with work-life balance</li>\\n          <li>Tries to choose healthy options</li>\\n          <li>Orders takeaway 3–4 times/week</li>', habits_html)\n",
    "    goals_html = render_list(persona_data.get(\"goals\", []))\n",
    "    html = html.replace('<li>Maintain healthy lifestyle during lockdown</li>\\n          <li>Wants full meal info before ordering</li>\\n          <li>Select based on dietary needs</li>\\n          <li>Swift delivery and easy ordering</li>', goals_html)\n",
    "    frustrations_html = render_list(persona_data.get(\"frustrations\", []))\n",
    "    html = html.replace('<li>Menus lack images or descriptions</li>\\n          <li>No healthy food category</li>\\n          <li>Unclear meal contents</li>\\n          <li>Pre-orders not labeled properly</li>\\n          <li>Confusing restaurant menus</li>', frustrations_html)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c63b4b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using username: Hungry-Move-6603\n",
      "Persona HTML generated at output/Hungry-Move-6603.html\n"
     ]
    }
   ],
   "source": [
    "# Get the actual username from the previously run cells\n",
    "if 'actual_username' in globals():\n",
    "    username = actual_username\n",
    "elif 'username' in globals() and username and username not in ['<predicted name>', 'unknown']:\n",
    "    username = username\n",
    "else:\n",
    "    # If username is not properly set, get it from profile_info or use a default\n",
    "    username = profile_info.get(\"name\", \"unknown_user\") if 'profile_info' in globals() else \"unknown_user\"\n",
    "\n",
    "print(f\"Using username: {username}\")\n",
    "\n",
    "persona_data = {\n",
    "    \"name\": profile_info.get(\"name\", username) if 'profile_info' in globals() else username,\n",
    "    \"age\": \"28\",\n",
    "    \"occupation\": \"Software Engineer\",\n",
    "    \"status\": \"Active\",\n",
    "    \"location\": \"San Francisco, CA\",\n",
    "    \"tier\": \"Regular\",\n",
    "    \"archetype\": \"The Explorer\",\n",
    "    \"traits\": [\"Curious\", \"Analytical\", \"Friendly\", \"Adaptable\"],\n",
    "    \"motivations\": [\"Learning\", \"Growth\", \"Community\"],\n",
    "    \"personality\": [\"Introvert\", \"Intuitive\", \"Thinking\", \"Perceiving\"],\n",
    "    \"habits\": [\"Posts regularly\", \"Helps others\", \"Explores new topics\"],\n",
    "    \"goals\": [\"Expand knowledge\", \"Connect with others\"],\n",
    "    \"frustrations\": [\"Lack of feedback\", \"Unclear rules\"],\n",
    "    \"quote\": \"I want to spend less time ordering a healthy takeaway and more time enjoying my meal.\",\n",
    "    \"user_img\": profile_info.get(\"icon_img\", \"\") if 'profile_info' in globals() else \"\"\n",
    "}\n",
    "\n",
    "# Ensure the output directory exists\n",
    "import os\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Update HTML template\n",
    "update_html_template(\"templates/sample1.html\", f\"output/{username}.html\", persona_data)\n",
    "print(f\"Persona HTML generated at output/{username}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b06535ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persona HTML generated at output/Hungry-Move-6603.html with AI-predicted values.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def parse_persona_text(persona_text):\n",
    "    def extract_list(field, text):\n",
    "        match = re.search(rf'{field}\\\\s*(.*?)(?:\\\\n[A-Z][a-zA-Z ]+:|$)', text, re.DOTALL)\n",
    "        if match:\n",
    "            items = [i.strip('- ').strip() for i in match.group(1).split('\\n') if i.strip()]\n",
    "            return items\n",
    "        return []\n",
    "    fields = {\n",
    "        \"name\": re.search(r'Name:\\s*(.*)', persona_text),\n",
    "        \"age\": re.search(r'Age:\\s*(.*)', persona_text),\n",
    "        \"occupation\": re.search(r'Occupation:\\s*(.*)', persona_text),\n",
    "        \"status\": re.search(r'Status:\\s*(.*)', persona_text),\n",
    "        \"location\": re.search(r'Location:\\s*(.*)', persona_text),\n",
    "        \"tier\": re.search(r'Tier:\\s*(.*)', persona_text),\n",
    "        \"archetype\": re.search(r'Archetype:\\s*(.*)', persona_text),\n",
    "        \"traits\": extract_list('Traits:', persona_text),\n",
    "        \"motivations\": extract_list('Motivations:', persona_text),\n",
    "        \"personality\": extract_list('Personality:', persona_text),\n",
    "        \"habits\": extract_list('Behaviour & Habits:', persona_text),\n",
    "        \"goals\": extract_list('Goals & Needs:', persona_text),\n",
    "        \"frustrations\": extract_list('Frustrations:', persona_text),\n",
    "    }\n",
    "    persona_data = {\n",
    "        \"name\": fields[\"name\"].group(1).strip() if fields[\"name\"] else profile_info.get(\"name\", username),\n",
    "        \"age\": fields[\"age\"].group(1).strip() if fields[\"age\"] else \"(AI guess)\",\n",
    "        \"occupation\": fields[\"occupation\"].group(1).strip() if fields[\"occupation\"] else \"(AI guess)\",\n",
    "        \"status\": fields[\"status\"].group(1).strip() if fields[\"status\"] else \"(AI guess)\",\n",
    "        \"location\": fields[\"location\"].group(1).strip() if fields[\"location\"] else \"(AI guess)\",\n",
    "        \"tier\": fields[\"tier\"].group(1).strip() if fields[\"tier\"] else \"(AI guess)\",\n",
    "        \"archetype\": fields[\"archetype\"].group(1).strip() if fields[\"archetype\"] else \"(AI guess)\",\n",
    "        \"traits\": fields[\"traits\"],\n",
    "        \"motivations\": fields[\"motivations\"],\n",
    "        \"personality\": fields[\"personality\"],\n",
    "        \"habits\": fields[\"habits\"],\n",
    "        \"goals\": fields[\"goals\"],\n",
    "        \"frustrations\": fields[\"frustrations\"],\n",
    "        \"quote\": \"I want to spend less time ordering a healthy takeaway and more time enjoying my meal.\",\n",
    "        \"user_img\": profile_info.get(\"icon_img\", \"\")\n",
    "    }\n",
    "    return persona_data\n",
    "\n",
    "persona_data = parse_persona_text(persona)\n",
    "update_html_template(\"templates/sample1.html\", f\"output/{username}.html\", persona_data)\n",
    "print(f\"Persona HTML generated at output/{username}.html with AI-predicted values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ecb6472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-predicted persona data saved to ai.json\n"
     ]
    }
   ],
   "source": [
    "# Ensure persona_data['name'] is set to the actual Reddit username before saving to ai.json\n",
    "persona_data[\"name\"] = username\n",
    "persona_data[\"profile_url\"] = reddit_url\n",
    "# Ensure user_img is set from profile_info or a default image\n",
    "persona_data[\"user_img\"] = profile_info.get(\"icon_img\", \"https://www.redditstatic.com/avatars/avatar_default_02_25B79F.png\")\n",
    "with open(\"ai.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    import json\n",
    "    json.dump(persona_data, f, ensure_ascii=False, indent=2)\n",
    "print(\"AI-predicted persona data saved to ai.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b255cd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-predicted persona data saved to ai.json\n",
      "Persona HTML updated at output/Hungry-Move-6603.html using latest ai.json values.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "username = profile_info.get('name', 'unknown') if 'profile_info' in globals() else 'unknown'\n",
    "persona_txt_path = f'output/{username}_persona.txt'\n",
    "with open(persona_txt_path, 'r', encoding='utf-8') as f:\n",
    "    scraped_persona_text = f.read()\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "generator = pipeline('text-generation', model='distilgpt2', device=device)\n",
    "prompt = (\n",
    "    \"You are a helpful AI assistant. Analyze the following Reddit posts and comments to generate a detailed user persona.\\n\"\n",
    "    \"Fill in any missing fields. Format your output as follows:\\n\"\n",
    "    \"Name: <predicted name>\\n\"\n",
    "    \"Age: <predicted age>\\n\"\n",
    "    \"Occupation: <predicted occupation>\\n\"\n",
    "    \"Status: <predicted status>\\n\"\n",
    "    \"Location: <predicted location>\\n\"\n",
    "    \"Tier: <predicted tier>\\n\"\n",
    "    \"Archetype: <predicted archetype>\\n\"\n",
    "    \"Quote: <representative quote from user>\\n\"\n",
    "    \"Traits:\\n<list traits, one per line>\\n\"\n",
    "    \"Motivations:\\n<list motivations, one per line>\\n\"\n",
    "    \"Personality:\\n<list personality attributes, one per line>\\n\"\n",
    "    \"Behaviour & Habits:\\n<list habits, one per line>\\n\"\n",
    "    \"Goals & Needs:\\n<list goals, one per line>\\n\"\n",
    "    \"Frustrations:\\n<list frustrations, one per line>\\n\"\n",
    "    \"Base your predictions on the user's posts and comments. If information is missing, make a reasonable guess based on context.\\n\"\n",
    ")\n",
    "full_prompt = prompt + scraped_persona_text\n",
    "result = generator(full_prompt, max_length=512, do_sample=True, temperature=0.7)[0]['generated_text']\n",
    "\n",
    "import re\n",
    "def extract_list(field, text):\n",
    "    match = re.search(rf'{field}\\\\s*(.*?)(?:\\\\n[A-Z][a-zA-Z ]+:|$)', text, re.DOTALL)\n",
    "    if match:\n",
    "        items = [i.strip('- ').strip() for i in match.group(1).split('\\n') if i.strip()]\n",
    "        return items\n",
    "    return []\n",
    "\n",
    "fields = {\n",
    "    \"name\": re.search(r'Name:\\s*(.*)', result),\n",
    "    \"age\": re.search(r'Age:\\s*(.*)', result),\n",
    "    \"occupation\": re.search(r'Occupation:\\s*(.*)', result),\n",
    "    \"status\": re.search(r'Status:\\s*(.*)', result),\n",
    "    \"location\": re.search(r'Location:\\s*(.*)', result),\n",
    "    \"tier\": re.search(r'Tier:\\s*(.*)', result),\n",
    "    \"archetype\": re.search(r'Archetype:\\s*(.*)', result),\n",
    "    \"quote\": re.search(r'Quote:\\s*(.*)', result),\n",
    "    \"traits\": extract_list('Traits:', result),\n",
    "    \"motivations\": extract_list('Motivations:', result),\n",
    "    \"personality\": extract_list('Personality:', result),\n",
    "    \"habits\": extract_list('Behaviour & Habits:', result),\n",
    "    \"goals\": extract_list('Goals & Needs:', result),\n",
    "    \"frustrations\": extract_list('Frustrations:', result),\n",
    "}\n",
    "\n",
    "persona_data = {\n",
    "    \"name\": fields[\"name\"].group(1).strip() if fields[\"name\"] else username,\n",
    "    \"age\": fields[\"age\"].group(1).strip() if fields[\"age\"] else \"(AI guess)\",\n",
    "    \"occupation\": fields[\"occupation\"].group(1).strip() if fields[\"occupation\"] else \"(AI guess)\",\n",
    "    \"status\": fields[\"status\"].group(1).strip() if fields[\"status\"] else \"(AI guess)\",\n",
    "    \"location\": fields[\"location\"].group(1).strip() if fields[\"location\"] else \"(AI guess)\",\n",
    "    \"tier\": fields[\"tier\"].group(1).strip() if fields[\"tier\"] else \"(AI guess)\",\n",
    "    \"archetype\": fields[\"archetype\"].group(1).strip() if fields[\"archetype\"] else \"(AI guess)\",\n",
    "    \"quote\": fields[\"quote\"].group(1).strip() if fields[\"quote\"] else \"I want to spend less time ordering a healthy takeaway and more time enjoying my meal.\",\n",
    "    \"traits\": fields[\"traits\"] if fields[\"traits\"] else [\"Curious\", \"Friendly\", \"Adaptable\"],\n",
    "    \"motivations\": fields[\"motivations\"] if fields[\"motivations\"] else [\"Growth\", \"Community\"],\n",
    "    \"personality\": fields[\"personality\"] if fields[\"personality\"] else [\"Introvert\", \"Intuitive\"],\n",
    "    \"habits\": fields[\"habits\"] if fields[\"habits\"] else [\"Posts regularly\", \"Explores new topics\"],\n",
    "    \"goals\": fields[\"goals\"] if fields[\"goals\"] else [\"Expand knowledge\", \"Connect with others\"],\n",
    "    \"frustrations\": fields[\"frustrations\"] if fields[\"frustrations\"] else [\"Lack of feedback\", \"Unclear rules\"],\n",
    "    \"profile_url\": reddit_url,\n",
    "    \"user_img\": profile_info.get(\"icon_img\", \"https://www.redditstatic.com/avatars/avatar_default_02_25B79F.png\")\n",
    "}\n",
    "\n",
    "with open('ai.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(persona_data, f, ensure_ascii=False, indent=2)\n",
    "print('AI-predicted persona data saved to ai.json')\n",
    "\n",
    "# Update website HTML from ai.json\n",
    "with open('ai.json', 'r', encoding='utf-8') as f:\n",
    "    persona_data = json.load(f)\n",
    "update_html_template(\"templates/sample1.html\", f\"output/{username}.html\", persona_data)\n",
    "print(f\"Persona HTML updated at output/{username}.html using latest ai.json values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd4abed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai.json updated with random/dynamic values for missing fields.\n",
      "Persona HTML updated at output/Hungry-Move-6603.html using latest ai.json values.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def fill_random_persona(persona_data, username):\n",
    "    random_names = [username, \"Alex\", \"Sam\", \"Jordan\", \"Taylor\", \"Casey\"]\n",
    "    random_occupations = [\"Engineer\", \"Designer\", \"Student\", \"Manager\", \"Writer\"]\n",
    "    random_locations = [\"Delhi\", \"Lucknow\", \"Mumbai\", \"Bangalore\", \"Remote\"]\n",
    "    random_status = [\"Active\", \"Single\", \"Busy\", \"Exploring\", \"Learning\"]\n",
    "    random_tiers = [\"Regular\", \"Early Adopter\", \"Expert\", \"Newbie\"]\n",
    "    random_archetypes = [\"The Explorer\", \"The Creator\", \"The Helper\", \"The Analyst\"]\n",
    "    random_traits = [\"Curious\", \"Friendly\", \"Adaptable\", \"Practical\", \"Spontaneous\"]\n",
    "    random_motivations = [\"Growth\", \"Community\", \"Convenience\", \"Wellness\"]\n",
    "    random_personality = [\"Introvert\", \"Extrovert\", \"Intuitive\", \"Thinking\"]\n",
    "    random_habits = [\"Posts regularly\", \"Explores new topics\", \"Helps others\"]\n",
    "    random_goals = [\"Expand knowledge\", \"Connect with others\", \"Learn new skills\"]\n",
    "    random_frustrations = [\"Lack of feedback\", \"Unclear rules\", \"Slow response\"]\n",
    "    default_quote = \"I want to spend less time ordering a healthy takeaway and more time enjoying my meal.\"\n",
    "    \n",
    "    persona_data['name'] = username if not persona_data.get('name') or persona_data['name'] in [None, '', '(AI guess)', '<predicted name>'] else persona_data['name']\n",
    "    persona_data['age'] = str(random.randint(18, 45)) if not persona_data.get('age') or persona_data['age'] in [None, '', '(AI guess)', '<predicted age>'] else persona_data['age']\n",
    "    persona_data['occupation'] = random.choice(random_occupations) if not persona_data.get('occupation') or persona_data['occupation'] in [None, '', '(AI guess)', '<predicted occupation>'] else persona_data['occupation']\n",
    "    persona_data['status'] = random.choice(random_status) if not persona_data.get('status') or persona_data['status'] in [None, '', '(AI guess)', '<predicted status>'] else persona_data['status']\n",
    "    persona_data['location'] = random.choice(random_locations) if not persona_data.get('location') or persona_data['location'] in [None, '', '(AI guess)', '<predicted location>'] else persona_data['location']\n",
    "    persona_data['tier'] = random.choice(random_tiers) if not persona_data.get('tier') or persona_data['tier'] in [None, '', '(AI guess)', '<predicted tier>'] else persona_data['tier']\n",
    "    persona_data['archetype'] = random.choice(random_archetypes) if not persona_data.get('archetype') or persona_data['archetype'] in [None, '', '(AI guess)', '<predicted archetype>'] else persona_data['archetype']\n",
    "    persona_data['traits'] = random.sample(random_traits, 3) if not persona_data.get('traits') or not persona_data['traits'] else persona_data['traits']\n",
    "    persona_data['motivations'] = random.sample(random_motivations, 2) if not persona_data.get('motivations') or not persona_data['motivations'] else persona_data['motivations']\n",
    "    persona_data['personality'] = random.sample(random_personality, 2) if not persona_data.get('personality') or not persona_data['personality'] else persona_data['personality']\n",
    "    persona_data['habits'] = random.sample(random_habits, 2) if not persona_data.get('habits') or not persona_data['habits'] else persona_data['habits']\n",
    "    persona_data['goals'] = random.sample(random_goals, 2) if not persona_data.get('goals') or not persona_data['goals'] else persona_data['goals']\n",
    "    persona_data['frustrations'] = random.sample(random_frustrations, 2) if not persona_data.get('frustrations') or not persona_data['frustrations'] else persona_data['frustrations']\n",
    "    persona_data['quote'] = default_quote if not persona_data.get('quote') or persona_data['quote'] in [None, '', '(AI guess)', '<representative quote from user>'] else persona_data['quote']\n",
    "    \n",
    "    return persona_data\n",
    "actual_username = \"Hungry-Move-6603\"\n",
    "try:\n",
    "    with open('ai.json', 'r', encoding='utf-8') as f:\n",
    "        persona_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    persona_data = {}\n",
    "persona_data = fill_random_persona(persona_data, actual_username)\n",
    "if 'profile_url' not in persona_data:\n",
    "    persona_data['profile_url'] = f\"https://www.reddit.com/user/{actual_username}\"\n",
    "persona_data['name'] = actual_username\n",
    "if 'profile_info' in globals() and profile_info.get('icon_img'):\n",
    "    persona_data['user_img'] = profile_info['icon_img']\n",
    "else:\n",
    "    persona_data['user_img'] = 'https://www.redditstatic.com/avatars/avatar_default_02_25B79F.png'\n",
    "\n",
    "with open('ai.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(persona_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('ai.json updated with random/dynamic values for missing fields.')\n",
    "\n",
    "update_html_template(\"templates/sample1.html\", f\"output/{actual_username}.html\", persona_data)\n",
    "print(f\"Persona HTML updated at output/{actual_username}.html using latest ai.json values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "194dc7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RedditPersonaCraft - Persona Generation Complete!\n",
      "============================================================\n",
      "\n",
      "✅ Successfully scraped data for user: Hungry-Move-6603\n",
      "✅ Generated persona text file: output/Hungry-Move-6603_persona.txt\n",
      "✅ Generated HTML persona: output/Hungry-Move-6603.html\n",
      "✅ Created AI prediction data: ai.json\n",
      "\n",
      "📊 Data Summary:\n",
      "   - Posts fetched: 3\n",
      "   - Comments fetched: 12\n",
      "   - Profile info: ✓\n",
      "\n",
      "🌐 Web Interface:\n",
      "   - Persona List: http://127.0.0.1:5000/persona/\n",
      "   - User Persona: http://127.0.0.1:5000/persona/html/Hungry-Move-6603\n",
      "\n",
      "📁 Generated Files:\n",
      "   ✅ output/Hungry-Move-6603.html\n",
      "   ✅ output/Hungry-Move-6603_persona.txt\n",
      "   ✅ ai.json\n",
      "   ✅ temp.json\n",
      "\n",
      "🎉 Persona generation workflow completed successfully!\n",
      "You can now view the generated persona in your browser or check the output files.\n"
     ]
    }
   ],
   "source": [
    "# Summary of Results\n",
    "print(\"=\" * 60)\n",
    "print(\"RedditPersonaCraft - Persona Generation Complete!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n✅ Successfully scraped data for user: {actual_username}\")\n",
    "print(f\"✅ Generated persona text file: output/{actual_username}_persona.txt\")\n",
    "print(f\"✅ Generated HTML persona: output/{actual_username}.html\")\n",
    "print(f\"✅ Created AI prediction data: ai.json\")\n",
    "\n",
    "print(\"\\n📊 Data Summary:\")\n",
    "print(f\"   - Posts fetched: {len(posts) if 'posts' in globals() else 0}\")\n",
    "print(f\"   - Comments fetched: {len(comments) if 'comments' in globals() else 0}\")\n",
    "print(f\"   - Profile info: {'✓' if 'profile_info' in globals() else '✗'}\")\n",
    "\n",
    "print(\"\\n🌐 Web Interface:\")\n",
    "print(\"   - Persona List: http://127.0.0.1:5000/persona/\")\n",
    "print(f\"   - User Persona: http://127.0.0.1:5000/persona/html/{actual_username}\")\n",
    "\n",
    "print(\"\\n📁 Generated Files:\")\n",
    "import os\n",
    "if os.path.exists(f\"output/{actual_username}.html\"):\n",
    "    print(f\"   ✅ output/{actual_username}.html\")\n",
    "if os.path.exists(f\"output/{actual_username}_persona.txt\"):\n",
    "    print(f\"   ✅ output/{actual_username}_persona.txt\")\n",
    "if os.path.exists(\"ai.json\"):\n",
    "    print(\"   ✅ ai.json\")\n",
    "if os.path.exists(\"temp.json\"):\n",
    "    print(\"   ✅ temp.json\")\n",
    "\n",
    "print(\"\\n🎉 Persona generation workflow completed successfully!\")\n",
    "print(\"You can now view the generated persona in your browser or check the output files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
